<!DOCTYPE html>
<html id="home" lang="en">
<head>
<meta charset=UTF-8" />
<!-- disable iPhone inital scale --> 
<meta name="viewport" content="width=device-width; initial-scale=1.0" /> <link rel="shortcut icon" href="favicon.ico">  
<!--[if lt IE 9]> 
 <!-- html5.js for IE less than 9 --> 
	<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
 <!-- css3-mediaqueries.js for IE less than 9 --> 
	<script src="http://css3-mediaqueries-js.googlecode.com/svn/trunk/css3-mediaqueries.js"></script>
<![endif]--> 
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-38190740-1']); _gaq.push(['_trackPageview']);
 (function() { var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
   ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
   var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s); })();
</script>
<link rel="stylesheet" href="../css/reset.css" type="text/css" />
<link rel="stylesheet" href="../css/isotype-style.css" type="text/css" media="screen" /> 
<link rel="stylesheet" href="../css/standard.css" type="text/css" />
<link rel="stylesheet" href="../css/buttons.css" type="text/css" /> 
<link rel="stylesheet" href="http://fonts.googleapis.com/css?family=Aclonica:regular" type="text/css" /> 
<TITLE>
AI & Robotics
</TITLE></HEAD>
<BODY>
<div id="pagewrap">
<header id="header">
 <hgroup><h1 id="site-logo">Spaces Between</h1></hgroup>
</header>
<DIV id="articleHeader">
<H3 class='entry-date'>2007-09-22 16:27:29</h3>
<H1 class="title">AI & Robotics</H1>
</DIV>
<div id="content">
<article class="post">
<h3>©1990-2007 Nick Porcino</h3> <hr size="3" />This paper was originally written in 1990 for presentation at Neural Information Processing Systems '91, but I left for Japan and so didn't submit it. <img src="../../posts/aiCloud.jpg"/> <small>Created with <a href="http://wordle.net">wordle.net</a></small> A lot has happened since 1990. The fields of computational neuroethology, neuromimetic control systems, and distributed control have advanced markedly. Since then, I've become involved with the entertainment industry and have been shipping games, not publishing papers. Peter Molyneux has this to say about the application of AI to games: <blockquote><em>The real trick now is to give the [gaming] world a realistic personality. </em><em>To achieve that, the gaming industry has to start concentrating on artificial intelligence. </em><em>Since the 1970s when the concept first was brought to the public eye, artificial intelligence has been completely out of fashion. Now, the amazing thing that's happened is that the entertainment industry is pulling artificial intelligence back into the limelight. The artificial intelligence problems of the 70s are now being cracked by scientists in the universities - and by game developers. </em> <em>Now, this is an absolutely incredible turn around in the computing industry. But the game designers have to drive this forward because if we're ever going to approach interactive movies, we're going to need artificial personalities. And to have artificial personalities, you need artificial intelligence. </em> <em>Up until that point in the process, every single thing has to be drawn and choreographed by an artist, but once you have artificial personalities then there are no limits - all of a sudden the world will start to generate itself. And this will mean a really true revolution in games. </em></blockquote> <hr />Keywords: distributed control, neural networks, subsumption, Braitenburg, games, artificial intelligence, robotics, mechatronics, neuroethology <hr /><strong>An Overview of the Methodology</strong> <strong>Presented here</strong> is a design methodology for distributed control of intelligent systems. Traditional task decomposition of control problems can lead to a combinatorial explosion in the complexity of a problem (the Complexity Trap), necessitating an alternative approach. The approach presented here offers a way out of the Complexity Trap by: <ol> <li>Distinguishing between high level behavioural control (meta-reflexes) and low level reflex behaviours</li> <li>Recognizing that complex behaviours may be the result of a complex environment</li> <li>Emphasizing the important distinctions between external environment sensing, intention, self-awareness, and peripheral sensing</li> <li>Mimicking the functional success of biological nervous systems</li> <li>Specifying that behaviours must be self-calibrating</li> <li>Promoting consensus decision making rather than absolute control by individual sub-systems</li> <li>The idea that behaviours and systems are reciprically related, rather than hierarchically.</li> </ol> The hierarchical Subsumption Architecture of (Brooks, 1986) is a decisive step towards a robust methodology for distributed control, but at the higher levels it falls victim to the same complex task decomposition and higher level design problems as the traditional methods. A typical criticism of the Subsumption Architecture is that the architecture is non-deterministic, entirely state-driven, and because it does not incorporate memory of actions, it cannot carry out predetermined sequences of actions (Bellingham, et al, 1990). The approach presented here combines many of the ideas of the Subsumption Architecture with neural network control principles, yielding an architecture for distributed intelligent control which is capable of exhibiting goal-directed behaviour and sequencing behaviours to achieve particular results. Following (Nagata, et al, 1990), my architecture splits functionality into two parts - a <tt>reason module</tt>, and an <tt>instinct module</tt>. Within each module "reflexes" are arranged according to neural network or Subsumption Architecture design methodologies. These reflexes implement simple or atomic tasks which can be easily developed and tested. The meta-reflexes of the reason module monitor external environment signals and the self awareness vector in order to select appropriate low-level reflex behaviours to be carried out by the instinct module's reflexes. This control is exercised via the intention vector. <img src="../../posts/syntFigure1.gif" /> The use of a desires automaton in the instinct module allows the sequencing of behaviours through the manipulation of self-awareness signals. Peripheral signals are used by the instinct module to modulate such basic reflexes as manipulator control, body orientation, locomotion, and so on. The use of the proposed architecture is demonstrated by Network 1 which accomplishes the task of multi-robot coordination, and by Network 2, the artificial insect controller of (Beer, et al, 1990) whose architecture is isomorphic with the work presented here. <strong>1. Introduction: Escaping the Complexity Trap</strong> AI has advanced as far as it has through the successful application and exploitation of Reductionism. Unfortunately, reductionism leads to the "Complexity Trap". Von Neumann (1958) said that "if you can describe something precisely enough in A implies B language, you can program it. " Accordingly, it should be possible to fully describe a task through a decomposition of "fine enough" detail. Unfortunately, as the early practitioners of AI soon discovered, task decomposition quickly becomes monstrous for a task of any significance. The difficulty, or even the impossibility, of making precise enough system descriptions is the Complexity Trap. The architecture proposed here is not so much a new architecture for distibuted control as it is a conceptual framework to facilitate the development of intelligent systems with a high degree of flexibility in their operation and in their interactions with underspecified environments. <h5>The Blockworld Problem</h5> Consider the plight of a Blockworld robot equipped with a video camera and an arm told to put the small red block on top of the green block (Minsky, 1988). At first, the task decomposition is fairly straight forward: find the small red block, find the green block, put the red block on top of the green one. The lurking complexity is revealed at the next level of decomposition: "find the small red block" becomes a question of selecting one object among many in the environment. The task breaks down into smaller and smaller steps until it finally reaches the level of basic manipulation of bit-mapped figures. How can a block be recognized in its environment? How can the arm be moved to a particular point in space? How can the concept of on top of be specified? The description of the problem soon becomes monstrous, the decomposition enormous. As it turns out, the functions of human intelligence which we consider the highest (such as logical reasoning) are the easiest for a computer to do. Integral calculus and logical proofs can be easier to program than "simple" tasks like distinguishing a circle from a square because at the lowest levels the calculus problems are logical operations on strings of symbols, whereas distinguishing circles from squares involves more difficult problems such as the fundamental representations of concepts and geometries (Minsky, 1988). <strong>Complex Behavior arises from the Environment</strong> The architecture presented here follows an alternative approach.Instead of aiming for a system description (which must almost necessarily be incomplete), simplex reflex arcs subserving low level behaviours are designed, and feedback is introduced to modulate the interactions of the behaviours. The resulting system exhibits so called "emergent behaviours" - behaviours which have not been explicitly specified at the level of the system's implementation. As von Neumann (1958) warned, the full description of a system's behaviour may be more complex than the system itself. Complex behaviour may be the result of a complex environment (Brooks, 1986). The architecture presented here was designed to exploit this idea. <strong>Internal and Peripheral Senses</strong> The difference in character between autonomous, strictly reactive reflexes and the more complex behaviours which are a result of the interactions of lower level reflexes suggests a functional split for the proposed architecture. Following (Nagata, et al, 1990) the architecture splits a system into an "instinct" module where all the simple reflexes exist autonomously, and a "reason" module comprised of what are essentially meta-reflexes operating in reaction to both signals from the instinct module and to environmental stimuli. The reason module sends an intention vector to the instinct module to modulate the low level reflexes. Study of architectures incorporating the reason-instinct split revealed that the two modules can be characterized in terms of the sensory modalities appropriate to each module. Some senses such as hearing and vision are specific to the external environment. Other senses are internal, for example, feedback about the state of low level reflexes, and proprioceptive sensing such as force feedback from a manipulator. Whereas external senses are of primary importance only to the reason module, the internal senses must be divided into a self-awareness vector for the reason module, and peripheral senses for the instinct module. The self-awareness signals are the means by which the reason module's meta-reflex can monitor (be aware of) the activity of the reflexes in the instinct module. The peripheral senses and intention vector drive the instinct module, which in turn controls the effectors, completing the loop by providing a means whereby a robotic or other intelligent system can modify, and be modified by, the total environment. Figure 1 summarizes the Level I architecture. <hr /> <h5>Figure 1</h5> <h5><img src="../../posts/syntFigure1.gif" /></h5> <ul>The Architecture is modelled after the invertebrate nervous system. The <strong>reason</strong> module is analogous to the invertebrate's cerebral ganglia, the <strong>instinct</strong> module corresponds to the thoracic or sub-cerebral ganglia. The <strong>intention vector</strong> is the means by which <strong>meta-reflexes</strong> control <strong>reflexes</strong>, and the <strong>self-awareness vector</strong> allows the reflexes to influence the meta-reflexes.</ul> <hr /> <h5>Neuronal Networks</h5> Neural and neuronal networks are important to the Architecture as a quick glance at the references reveals. In fact, the Architecture grew out of a study of invertebrate nervous systems and the application of neuromimetic computational architectures to intelligent systems control (Porcino 1990, Porcino and Collins, 1990). Invertebrates exhibit a high level of sophistication despite the relative simplicity of their nervous systems, and thus pose a significant challenge to researchers (Cliff, 1990). Furthermore, invertebrate neuronal networks are self-calibrating, like the behaviours intended for the Subsumption Architecture. Invertebrate neuronal networks are very much like adaptive control systems, and are ideally adapted by Nature for realtime control. Artificial neural networks attempt to mimic the functional success of biological nervous systems. Most neural network methods attempt to model large numbers of regularly connected identical neurons with no feedback. Typically gradient descent is performed on an associated Lyapunov energy function, and through this process a neural network converges on a solution. Traditional neural computational structures are only now beginning to demonstrate some competence in the termporal domain, but they are still unwieldy as they are still only functional black boxes with well defined input-output relations - signals are pipelined straight through with no internal feedback. In contrast, the approach taken in the development of the Architecture presented here differs in that inspiration was taken from the study of simple invertebrate neuronal networks which in general have small numbers of dedicated purpose neurons whose interconnections are fairly easy to ascertain or deduce. <h4><strong>2. Distributed Control By Invertebrate Neuronal Networks </strong></h4> Returning again to von Neumann's warning about complexity, we can deduce that if the full description of a system's behaviour is more complex than the system itself, it follows that many abilities will not be directly specified in a full system description. In other words, complex behaviours emerge from the interactions of environment and the simpler elements making up the system. Theoretical system models often capture the flavour of some small aspect of a system's behaviour, but these models usually consider only very restricted contexts (cf. Guthrie, 1980). In actual fact the sheer multiplicity of behaviours and interactions between behaviours is natrual organisms are too complex for full description. Similarly, the behaviour of useful intelligent systems is equally hard to describe. Most current robotic systems operate in highly constrained environments such as the factory floor where only small numbers of predefined task need be performed on certain types of objects. The environment can be explicitly controlled; precisely defined objects are found only in specified places, thereby facilitating reliability, determinisim, speed, and operational efficiency. It is far easier to describe a very restricted environment and the behaviours possible in that environment than it is to specify open-ended real world systems. Nonetheless, modern intelligent systems have to deal with humans and more natural, less well-behaved environments. Inspired by the evolutionary progress of natrual systems, the neuro-mimetic computational structure is highly parallel, locally simple, and robust in the face of unexpected situations or even systemic failures. In natural nervous systems, no single processing unit controls behaviour - there is a progression of behavioural complexity beginning with the basic reflex arcs governed by a relatively small group of neurons (behaviours such as simple rhythmic motions or withdrawing from aversive stimuli), and ranging to the more complex behaviours governed by the higher nuclei and cortex (such as feeding and social interaction). Natural nervous systems are reciprocally connected and build consensus (Figure 3). They exhibit simple stereotyped behaviours generated by simple reflexes, and the interactions of simple reflexes lead ultimately to sophisticated behavioural repetoires. These concepts for distributed architectures imply: <ol> <li>Redundancy and Robustness because responsibility for any particular behaviour resides in no single unit</li> <li>Self Calibration of Behaviours through tight sensory feedback</li> <li>Behaviours which outlast the triggering stimulus through the action of feedback signals</li> <li>Command locus near the site of incoming information for maximum processing speed and minimum connection lengths and communication times (Davis, 1976)</li> <li>Specialization of units near sensory input due to self organization of neural circuits in reaction to patterns in sensory input (Davis, 1976); and</li> <li>Optimization and self adaptation of behaviours resulting from cooperation and competition between units (cf. Klopf, 1988, Grossberg, 1987).</li> </ol> <h4><strong>3. Reflexes and Meta-Reflexes </strong></h4> If the head of an insect is severed, reflexes governed by ganglia lower than the cerebral ganglia are often released. Leeches swim continuously. Cockroaches walk spontaneously. Nereis will spontaneously creep, and it no longer withdraws if tapped. Female mantises bite off the head of the male to disinhibit copulatory behaviour. It seems that meta-reflexes in the cerebral ganglia control the reflexes of the lower ganglia. The reason-instinct split is suggested by this commonly observed nervous organization. The utility of meta-reflexes is simply illustrated by an example. One of the most apparently complicated behavioural displays is the group action of a large number of organisms, whether it is the flocking of birds, the schooling of fish, the milling of crowds, or the herding of mammals. When a flock of birds wheels overhead, it seems there must be some sort of central coordinating intelligence or some wonderfully complex communication system to allow all the birds to swoop and swirl around each other without ever crashing, or for all the birds to begin a turn in the same direction at almost the same time. As it turns out, no "super" control or communication is necessary. The flocking behaviour can be imitated by a very simple controller local to every member of the flock. Consider Network 1 (Figure 4). This behavioural controller, cast in the Architecture developed here's mould, is reminiscent of Braitenberg's Vehicles (1984) and Reynolds' Boids (1987). Figure 5 shows the behaviour of simulated vehicles using the controller. Notice that in general the vehicles attempt to reach the goal, but swerve to avoid collisions. A number of similar examples in three dimensions can be found in (Reynolds, 1987) and (Wilhelm and Skinner, 1990). The Wilhelm and Skinner paper elaborates on an interactive design system for Boid architectures, called Notions. The robots are given simple bodies equipped with some means of locomotion - wheels or legs, and some rudimentary senses. In Figure 3, sensors are drawn in left-right pairs pointed a few degrees off center. Each half of a sensor pair is sensitive to stimuli that are strongest in the direction the sensor is pointing. As the stimuli is moved off the sensor's axis, sensor response falls off logarithmically. This sensitivity pattern assures that the sensor whose axis passes nearest the stimuli will respoind the strongest. If a stimuli is "dead ahead, " both sensors respond equally. Logarithmic sensor response is used throughout the Architecture. Sensors are designed so that behaviourally significant states evoke a high resopnse, but response falls off quickly as the significant state is left. Logarithmic sensor response ensures that significant states evoke an impreative signal to reflexes and meta-reflexes (Porcino, 1990). The highest activations of a sensor indeicate the most imperative conditions in a fashion analogous to pain. This analogy points the way to learning in these circuits, perhaps using drive-reinforcement learning as described in (Klopf, 1988). Consider the instinct module of Network 1. Locomotion receives peripheral sensing signals specific to the mode of locomotion - for example, slip sensors for wheels, load sensors for legs. In addition, locomotion receives an intention vector consisting of a turn left magnitude, and a turn right magnitude. Based on the difference between the two inputs, the robot turns and moves forward. The reason module introduces the concept of "consensus decision building." The intention vector is the sum of the sum turn left and sum turn right processes. These processes simply pass on a weighted sum of their inputs, the simplest possible form of consensus. The inputs on the sum turn processes come from three meta-reflexes: avoid collisions, move to center, and move to goal. Move to goal is defined according to Reynolds (1987): it attempts to move a flock member to the center of its nearest flockmates and also to match the average velocity of the nearest flock mates. Move to goal causes a taxis towards some "homing" point that can be used to steer and direct the flock. Avoid collisions steers a flock member away from imminent disaster. Avoiding collisions has the highest behavioural priority, so avoid collisions ensures that its outputs dominate sum turn in times of crisis by shutting off the other two meta-reflexes. On the other hand, neither move to center nor move to goal is more important than the other, so sum turn forms a consensus between the two. <h4><strong>4. Self Awareness </strong></h4> Network 1 illustrates the use of reflexes and meta-reflexes. It is ia controller which could be easily implemented by Levels 0, 1, and 2 of the Subsumption Architecture (Figure 6). Like the Architecture presented here, the Subsumption Architecture makes use of networks of simple interacting reflexes (Brooks, 1986). Higher level Subsumption Architecture behaviours supproess lower level behaviours and take over their functionality in a process called "subsumption." Lower level behaviours can never suppress higher level behaviours. In contrast, the Architecture presented here avoids a hierarchical arranagement by introducing the Self-Awareness feedback vector, which allows the emergence of interesting behaviour. <h5>Limitations of the Subsumption Architecture</h5> The principal advantage of Subsumption is that low level behaviours can be readily designed and implemented, and subsequencty layers can be added on top of the correctly functioning lower levels. Unfortunately, there are some implicit problems with the Subsumption Architecture which have been addressed with varying degrees of success: <ol> <li>Since the Subsumption Architecture's hierarchical structure is strictly reductionist, the higher level behaviours fall victim to the task decomposition problems inherent in the Complexity Trap (Brooks, 1986, section IIC);</li> <li>It is completely reactive to the environment: its behaviour is nondeterministic and entirely state-driven. (Bellingham et al, 1990);</li> <li>It does not incorporate memory of actions and so cannot carry out predetermined sequences of events (Bellingham et al, 1990).</li> </ol> The Architecture presented here addresses these problems through the introduction of the self awareness vector and by allowing reflexes and meta-reflexes to work through reciprocity instead of hierarchy. In order to illustrate these principles we willl consider the simulated insect behavioural controller of (Beer, Chiel, and Sterling, 1990). This controller has been selected because it is isomorphic to the Architecture presented here (Network 2, Figure 7). The behaviour of this controller is well understood and exhibits a number of properties which suggest that controllers using similar architectures might be suitable for the control of other intelligent autonomous systems. The basis reflexes of Network 2 are the locomotion and chewing reflexes, these suggest the two basic behaviours of the simulated insect: moving around and eating. The reciprocally connected meta-reflexes are eat, move to food, follow edges, and wander. The simulated insect's instinct module has an internal energy monitor which watches the energy level of the insect. Like the senses of Network 1, the energy monitor has a logarithmic response, becoming most active in the most behaviourally significant state: when the internal energy state is low, the insect is "hungry." As the insect gets "hungry" the eat and move to food meta-reflexes become more and more active. Move to food suppresses follow edges, although wander continues to be active. Move to food operates in the same way as the move to goal reflex of Network 1, that is, by following "scent" gradients detected by two antenn・ When "food" is found, the sensors in the mouth activate the chew reflex. The chew reflex in turn suppresses the move to food reflex and allows the eat reflex to become active which further suppresses move to food. Eating continues until the internal energy level is saturated, at which point the internal energy monitor becomes inactive, releasing the follow edges behaviour from inhibition. Eat and move to food also become inactive. This controller is somewhat similar to systems implementing competence levels 0, 1, and 2 of the Subsumption Architecture. However, the low levels of that architecture are strictly reactive to environmental stimuli. This controller, although no more complex than a simple Subsumption Architecture system exhibits goal directed behaviour in addition to its reflexive behavioural repetoire - it searches for "food" whenever the internal energy store is low. Network 2 uses reciprocity and consensus to reach decisions. A hierarchy is simpler to analyze and perhaps simpler to program, but it drastically limits the complexity of the behaviours a system can exhibit. Unlike the Subsumption Architecture, Network 2 exhibits several of the characteristics of motivated behaviour as defined in (Kupfermann, 1974): <ol> <li>The controller groups and sequences behaviours (to obtain food when hungry);</li> <li>It is goal directed (it generates movements which serve to find food);</li> <li>It exhibits changes in behaviour based upon an internal state (it attempts to find food when hungry, and ignores food otherwise);</li> <li>Behaviours can persist if stimuli are removed (if food is removed or runs out while the insect is eating, appetitive behaviour persists.</li> </ol> Simulation shows that the distributed nature of the system allows it to cope with complex and underspecified environments (Beer, Chiel, and Sterling, 1990). As we have come to expect from neuromimetic systems, if the system is damaged, performance degradation is gradusal (Chiel and Beer, 1989). Note especially that the full behaviour of the system is not explicitly defined anywhere in the system, emerging instead from the interactions between simple behaviours. Through the principles of emergecne, the Complexity Trap is avoided. <h4><strong>5. The Desires Automaton </strong></h4> To be fully practical, a control architecture must be able to specify exact sequences of behaviours. In order to give Level 1 this ability, the desires automaton is introduced. It is essentially a stored sequence generator. Its action is to activate specific meta-reflexes then wait for a terminating condition to be satisifed, at which point it activates another set of meta-reflexes. The name desires comes from the fact that the automaton acts through the self-awareness vector to tell the meta-reflexes what the "want" to do next. The simplest mechansim to implement the desires automaton is a finite state machine or a Petri net. The desires automaton has its biological precedent in the low level behavioural sequence reflexes of invertebrates which simple mindedly carry out complex tasks such as taking steps, building webs, spinning cocoons, bring prey back to a nest, mating, and so on. It seems that many of these tasks are sequenced by ganglia at a level lower than the cerebral ganglion and thus appear to be reflexes controlling meta-reflexes. These meta-meta-reflexes foreshadow the "association" module of Level 2. <h4><strong>Conclusion: Future Directions </strong></h4> Level 0 networks are analogous to extremely simple nervous systems such as those found in sea creatures such as sponges, starfish, and jellyfish. Level 1 networks are analogous to higher invertebrate nervous systems in organisms like crustaceans and insects. Level 0 could be considered to be the instinct module working alone, Level 1, instinct and reason working together. The introduction of a self-awareness feedback vector allows Level 1 to exhibit many of the characteristics of motivated behvaoiur. In addition, the addition of a desires finite state automaton to the reason module allows sequences of behaviours to be programmed into the system. In this way, some of the limitations of the Subsumption Architecture are overcome. The availability of alarge body of neuroethological literature means that prototypes and guides for the design of new intelligent systems are readily available (cf. Guthrie, 1980; Davis, 1976). Currently under development, Level 2 adds a higher order "association-experience" module which is analogous to the cerebellum and cortex of vertebrate nervous systems, see Figure 8. The association module introduces meta-meta-reflexes implemented in a regularized neural network architecture, examples of which abound in the literature. The meta-meta-reflexes incorporate memory of internal and external states, awareness of duration, and awareness of the activity of the meta-reflexes. The meta-meta-reflexes are able to exhibit behaviours such as boredom if some meta-reflex is active for a long time, and impatience if a desired goal is not forthcoming. A problem which has long stymied neural network research is the incorporation of temporal behaviour. Intelligent systems architected as described in this paper provide a temporal and physical context for neural networks in which temporal behaviour is a natural and implicit mode of operation. It is my contention that temporal and physical context coupled with the Architectures as introduced here will yield systems well able to cope with their environments. Furthermore, the association-experience level of Level 2 is where adaptive, intelligent behaviour will arise. <h4>References:</h4> Note: These references are shared amongst all files on this site. Barrington, E.J.W., <strong>Invertebrate Structure and Function</strong>, Thomas Nelson and Sons, Don Mills, Ont., 1969 in Locomotion in Terrestrial Arthropods, pp. 134-141) Bassler, Ulrich, <strong>Proprioceptive Control of Stick Insect Walking</strong> in Insect Locomotion, Michael Gewecke and Gernot Wendler (eds.), Verlag Paul Parey, 1985, pp. 43-48 Beer, Randall B., Hillel S. Chiel, and L.S. Sterling, <strong>A Biological Perspective on Autonomous Agent Design,</strong> 1990 Beer, Randall B., Hillel S. Chiel, and L.S. Sterling, <strong>Heterogeneous Neural Networks for Adaptive Behaviour in Dynamic Environments,</strong> in Advances in Neural Information Processing Systems, Volume 1, Morgan Kaufman Publishers, 1989 Bellingham, James G., Thomas R. Consi, Robert M. Beaton, William Hall, <strong>Keeping Layered Control Simple,</strong> Proceedings of the IEEE Symposium on Autonomous Underwater Vehicle Technology, June 5 and 6, 1990, Washington D.C., pp. 3-8 Braitenburg, Valentino, <strong>Vehicles: Experiments in Synthetic Psychology, </strong>MIT Press, Cambridge, 1984 Brooks, Rodney A., <strong>A Robust Layered Control System for a Mobile Robot, </strong>IEEE Journal of Robotics and Automation, vol. RA-2, no. 1, March 1986, pp. 14-23 Brooks, Rodney A., <strong>A Robot That Walks: Emergent Behaviours from a Carefully Evolved Network, </strong>Proc. IEEE Intl. Conf. on Robotics and Automation, vol. 2, pp. 692-696 Chiel, Hillel J., and Randall D. Beer, <strong>A Lesion Study of a Heterogenous Artificial Neural Network for Hexapod Locomotion,</strong> in Proccedings of the International Joint Conference on Neural Networks, 1989, pp. I: 407-414 Cliff, D. T., <strong>Computational Neuroethology: A Provisional Manifesto,</strong> Cognitive Science Research Paper, Serial No. CSRP 162, May 1990, The University of Sussex, School of Cognitive and Computing Science, Falmer, Brighton BN1 9QN, England, UK Davis, W.J., <strong>Cerebral Organization of Invertebrates,</strong>, in Neural Control of Locomotion, R.M. Herman, S.Grillner, P.S.G. Stein, and D.G. Stuart (Eds)., Plenum Press, New York, 1976, pp. 265-292 Dean, Jeffrey, <strong>A Simulation of Proprioceptive Input from the Coxal Hair Rows of the Stick Insect: Possible Effect of Step Velocity on the Representation of Joint Angle,</strong> in Insect Locomotion, Michael Gewecke and Gernot Wendler (eds.), Verlag Paul Parey, 1985, pp. 49-57 Gewecke, Michael <strong>Swimming Behaviour of the Water Beetle Dytiscus Marginalis L. (Coleoptera, Dytiscidae)</strong> in Insect Locomotion, Michael Gewecke and Gernot Wendler (eds.), Verlag Paul Parey, 1985, pp. 111-120 Gray, James, <strong>Animal Locomotion</strong>, Weidenfeld and Nicolson, London, 1968 Grossberg, Stephen, <strong>Competetive Learning: From Interactive Activation to Adaptive Resonance,</strong> Cognitive Science, 1987, vol. 11, pp. 23-67 Guthrie, D.M., <strong>Neuroethology: An Introduction,</strong> Blackwell Scientific Publications, Oxford, 1980 Hirose, Shigeo, <strong>A Study of a Design and Control of a Quadruped Walking Vehicle</strong>, Intl J of Robotics Research, vol. 3, no. 2, Summer 1984, pp. 113-133 Hoyle, Graham, <strong>Arthropod Walking,</strong> in Neural Control of Locomotion, R.M. Herman, S. Grillner, P.S.G. Stein, & D.G. Stuart (eds), Plenum Press, New York, 1976, pp. 137-179 Hustert, Reinhold, <strong>The Contribution of Proprioceptors to the Control of Motor Patterns of Legs in Orthopterous Insects - The Locust Example,</strong> in Insect Locomotion, Michael Gewecke and Gernot Wendler (eds.), Verlag Paul Parey, 1985, pp. 59-67 Jander, Jorg Peter, <strong>Mechanical Stability in Stick Insects When Walking Straight and Around Curves,</strong> in Insect Locomotion, Michael Gewecke and Gernot Wendler (eds.), Verlag Paul Parey, 1985, pp. 33-42 Klopf, A. Harry, <strong>A Neuronal Model of Classical Conditioning,</strong> Psychobiology, 1988, vol. 16, no. 2, pp. 85-125 Laurent, Gilles, & Reinhold Hustert, <strong>Motor Neuronal Receptive Fields Delimit Patterns of Motor Activity During Locomotion of the Locust</strong>, J. of Neuroscience, vol. 8, no. 11, Nov. 1988, pp. 4349-4366 Minsky, Marvin, <strong>The Society of Mind,</strong> Picador, London, 1987 Nagata, Shigemi, Minoru Sekiguchi, Kazuo Asakawa, <strong>Mobile Robot Control by a Structured Hierarchical Neural Network,</strong> IEEE Control Systems Magazine, April 1990, pp. 69-76 Pearson, Keir G., <strong>The Control of Walking</strong>, Scientific American, vol. 276, Dec. 1976 Pearson, Keir G., Robert F. Franklin <strong>Characteristics of Leg Movements and Patterns of Coordination in Locusts Walking on Rough Terrain, </strong>Intl J. of Robotics Research, vol. 3, no. 2, 1984, pp. 102-112 Porcino, Nick, <strong>A Neural Network Controller for Hexapod Locomotion,</strong> Proceedings of the International Joint Conference on Neural Networks, San Diego, 1990, pp. I:189-194 Porcino, Nick, and James S. Collins, <strong>An Application of Neural Networks to the Control of a Free Swimming Submersible,</strong> Proceedings of the International Joint Conference on Neural Networks, Washington, D.C., 1990, pp. II:417-420 Reynolds, Craig W., <strong>Flocks, Herds, and Schools: A Distributed Behavioural Model, </strong>ACM Computer Graphics, vol. 21, no. 4, July 1987 Reynolds, Craig W., <strong>Steering Behaviors For Autonomous Characters, </strong><a href="http://hmt.com/cwr/steer">http://hmt.com/cwr/steer</a>, 1997 von Neumann, John, <strong>The Computer and the Brain</strong>, MIT Press, 1958 Wilhelm, Jane, and Robert Skinner, <strong>A "Notion" for Interactive Behavioural Animation Control,</strong> IEEE Computer Graphics and Applications, May 1990, pp. 14-22
</article>
</div>
<div id="tags">
AI___Robotics/games/insect_AI/writing</tags>

<footer id="footer"> 
	<p>Content by Nick Porcino (c) 1990-2011</p> 
</footer> 
	<!-- /#footer --> 
</div>

</BODY>
</HTML>
